services:
    llama-cpp:
        image: llama-cpp-cpu
        container_name: llama-cpp
        build:
            dockerfile: Dockerfile
            context: .
            network: host
        volumes:
            - ~/ml/llm-models/:/models
        ports:
            - "8010:8010"
        command: [
            ./bin/server,
            --host, "0.0.0.0",
            --port, "8010",
            --ctx-size, "2048",
            --model, /models/phi-2.Q4_K_M.gguf,
        ]
        stop_grace_period: 2s
        tty: true
